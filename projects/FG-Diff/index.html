<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection</title>
  <link rel="icon" type="image/x-icon" href="">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->            
              <span class="author-block">
                <a href="https://xiaofeng-tan.github.io/" target="_blank">Xiaofeng Tan</a><sup>1,2</sup>,</span>
                <span class="author-block">
                <a >Hongsong Wang</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a >Xin Geng</a><sup>1,2</sup>
                </span>
                <span class="author-block">
                  <a >Liang Wang</a><sup>3,4</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> Department of Computer Science and Engineering, Southeast University, Nanjing, China</span>
                    <span class="author-block"> <sup>2</sup> Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications</span>
                    <span class="author-block">  <sup>3</sup> New Laboratory of Pattern Recognition (NLPR),  State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA)</span>
                    <span class="author-block"> <sup>4</sup> School of Artificial Intelligence, University of Chinese Academy of Sciences </span>
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2412.03044" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>ArXiv</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Xiaofeng-Tan/FGDMAD-Code?tab=readme-ov-file" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Motivation </h2>
      <img style="width: 70%; height: auto; display: block; margin: 0 auto;" src="static/images/intro.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-left">
        The data illustration. (a) The training and testing data, where the training data is composed of seen normal motions and the testing data contains unseen normal and abnormal motions. Although seen and unseen motions represent the same action (e.g., walking), their local details, such as stride length, arm swing amplitude, and joint angles, exhibit significant differences. (b) The frequency analyses of motions. This analysis reveals that a motion retaining only 70\% of its low-frequency information remains largely similar to the original motion in terms of global structure, with minor differences observed in the low-frequency regions. Note that low-frequency and high-frequency regions do not correspond directly to specific joints. Instead, low-frequency regions are defined as areas where joints predominantly contain low-frequency information while also exhibiting a relatively higher proportion of high-frequency details.
      </h2>
      <br>
      <h2 class="title is-3"> Method</h2>
      <img style="width: 70%; height: auto; display: block; margin: 0 auto;" src="static/images/intro_method.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-left">
        Comparison between our proposed method (green) and existing methods (blue). During the training phase, we employ adversarial training for the perturbation generator and denoiser to enhance model robustness. Specifically, the perturbation generator attacks the observed motion, producing motions that are challenging to reconstruct yet resemble normal motions. These perturbed motions are then used to train the denoiser, thereby improving its robustness. During the inference phase, we apply DCT to separate observed motion into global and local components, represented as low-frequency and high-frequency information. By leveraging high-frequency information as guidance, our method can accurately reconstruct observed motion compared to existing methods.
      </h2>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video anomaly detection (VAD) is a vital yet complex open-set task in computer vision, commonly tackled through reconstruction-based methods. However, these methods struggle with two key limitations: (1) insufficient robustness in open-set scenarios, where unseen normal motions are frequently misclassified as anomalies, and (2) an overemphasis on, but restricted capacity for, local motion reconstruction, which are inherently difficult to capture accurately due to their diversity. To overcome these challenges, we introduce a novel frequency-guided diffusion model with perturbation training. First, we enhance robustness by training a generator to produce perturbed samples, which are similar to normal samples and target the weakness of the reconstruction model. This training paradigm expands the reconstruction domain of the model, improving its generalization to unseen normal motions. Second, to address the overemphasis on motion details, we employ the 2D Discrete Cosine Transform (DCT) to separate high-frequency (local) and low-frequency (global) motion components. By guiding the diffusion model with observed high-frequency information, we prioritize the reconstruction of low-frequency components, enabling more accurate and robust anomaly detection. Extensive experiments on five widely used VAD datasets demonstrate that our approach surpasses state-of-the-art methods, underscoring its effectiveness in open-set scenarios and diverse motion contexts.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-3">Framework </h2>
      <img src="static/images/framework.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-left">
        <p>The framework of the proposed method. The model is trained utilizing generated perturbation examples. The training phase includes two processes: minimizing the mean square error to train the noise predictor and maximizing this error to train the perturbation generator. During the testing phase, the high-frequency information of observed motions and the low-frequency information of generated motions are fused for effective anomaly detection.</p>
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3"> Illustration of Perturbation Training </h2>
      <img src="static/images/perturb.png" alt="MY ALT TEXT" 
      style="width: 50%; height: auto; display: block; margin: 0 auto;"/>
      <h2 class="subtitle has-text-left">
        <p>The illustration of perturbation training. In Fig. (a), the green and yellow points denote the original training $x_k$ and perturbed motion $\hat{x}_k$, respectively. The red region represents the distribution of unseen normal samples. Accordingly, Fig. (b) demonstrates that the reconstruction domain is extended by our proposed perturbation training.</p>
      </h2>
    </div>
  </div>
</section>

<!-- End image carousel -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3"> Visualizations </h2>
      <h2 class="subtitle has-text-left">
        <p>The left videos are frame-level ground true label, and the right ones are detect results.</p>
      </h2>
      <div class="item item-video1">
        <video  poster="" id="video1" autoplay controls muted loop>
          <source src="static/videos/video_1.mp4" type="video/mp4">
        </video>
      </div>
      <div class="item item-video2">
        <video  poster="" id="video2" autoplay controls muted loop>
          <source src="static/videos/video_2.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Experiment Results </h2>
      <br>
      <h2 class="subtitle has-text-left">
        Comparison of the proposed method against other SoTA methods. The best results across all methods are in bold, the second-best ones are underlined, and the superscript \textsuperscript{â€¡} denotes the best performance across all the methods under each paradigm.
      </h2>
      <img style="width: 90%; height: auto; display: block; margin: 0 auto;" src="static/images/t1.png" alt="MY ALT TEXT"/>
      
      <br>

      <h2 class="subtitle has-text-left">
        Robust analysis of perturbations training. ``PT'' denotes perturbations training.``$\lambda_{PI}$'' represents the perturbations intensity in inference.
      </h2>
      <img style="width: 50%; height: auto; display: block; margin: 0 auto;" src="static/images/t2.png" alt="MY ALT TEXT"/>

      <br>

      <h2 class="subtitle has-text-left">
        Sensitivity analyses of DCT-Mask threshold $\lambda_\text{dct}$.
      </h2>
      <img style="width: 50%; height: auto; display: block; margin: 0 auto;" src="static/images/sen.png" alt="MY ALT TEXT"/>

      <br>

      <h2 class="subtitle has-text-left">
        Anomaly score curves on the Avenue and HR-UBnormal datasets. (a) Avenue dataset; (b) HR-UBnormal dataset. The horizontal axis represents the frame index, the red circles in the clip of each figure denote the abnormal events, and the green circles represent the normal ones.
      </h2>
      <img style="width: 70%; height: auto; display: block; margin: 0 auto;" src="static/images/vis.png" alt="MY ALT TEXT"/>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
