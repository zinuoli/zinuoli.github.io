---
layout: post
title: Our paper "TriSense" has been accepted to NeurIPS 2025!
date: 2025-09-22 16:11:00-0400
inline: false
related_posts: false
---

ðŸŽ‰ Excited to announce that our paper **"Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM"** has been accepted to **NeurIPS 2025**!

This work introduces TriSense, a novel multimodal large language model that can understand audio-visual-speech moments in videos. Our approach combines visual, audio, and speech modalities to provide comprehensive video understanding capabilities.

Special thanks to all my collaborators: Xian Zhang, Yongxin Guo, Prof. Mohammed Bennamoun, Prof. Farid Boussaid, Girish Dwivedi, Luqi Gong, and Dr. Qiuhong Ke.

ðŸ“„ [Paper](https://arxiv.org/pdf/2505.18110) | ðŸ’» [Code](https://github.com/zinuoli/TriSense)